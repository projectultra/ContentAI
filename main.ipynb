{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain pinecone-client sentence-transformers streamlit PyMuPDF bitsandbytes langchain-pinecone pinecone-client langchain_huggingface langchainhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchainimport fitz  # PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-community"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "#extract text from pdf\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    document = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "pdf_paths = [\"GOOG.pdf\", \"TSLA.pdf\", \"UBER.pdf\"]\n",
    "texts = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load the embedding model\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# chunk the doc text into smaller pieces\n",
    "\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1024,\n",
    "    chunk_overlap  = 20 # number of tokens to overlap\n",
    ")\n",
    "\n",
    "docs = []\n",
    "for text in texts:\n",
    "    docs.append(text_splitter.split_text(text))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually chunk the text for granular control\n",
    "\n",
    "def split_text(text, chunk_size=500, overlap=10):\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for paragraph in text.split('\\n'):\n",
    "        if current_length + len(paragraph) > chunk_size:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = current_chunk[-overlap:]  # Overlap\n",
    "            current_length = sum(len(p) for p in current_chunk)\n",
    "        current_chunk.append(paragraph)\n",
    "        current_length += len(paragraph)\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "chunk_size = 500\n",
    "overlap = 10\n",
    "chunked_texts = [split_text(text, chunk_size, overlap) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the chunks using embedding model\n",
    "\n",
    "chunked_embeddings = []\n",
    "for chunks in docs:\n",
    "    embeddings = embedding_model.encode(chunks)\n",
    "    chunked_embeddings.append(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vectors for pinecone\n",
    "\n",
    "vectors = []\n",
    "for doc_id, (chunks, embeddings) in enumerate(zip(docs, chunked_embeddings)):\n",
    "    if doc_id == 0:\n",
    "        company = \"GOOG\"\n",
    "    elif doc_id == 1:\n",
    "        company = \"TSLA\"\n",
    "    elif doc_id == 2:\n",
    "        company = \"UBER\"\n",
    "    for chunk_id, (chunk, embedding) in enumerate(zip(chunks, embeddings)):\n",
    "        vector_id = f\"{doc_id}_{chunk_id}\"\n",
    "        vectors.append((vector_id, # id to uniquely identify the vector\n",
    "                        embedding,\n",
    "                        {\"text\": chunk,\n",
    "                         \"company\": company}\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# create a pinecone object\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"], environment=\"us-west1-gcp\", serverless=ServerlessSpec(\"us-west1-gcp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index name to store the vectors\n",
    "\n",
    "index_name = \"contentai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an index\n",
    "\n",
    "pc.create_index(\n",
    "    name=index_name,\n",
    "    dimension=384, # dimension of the embedding vectors\n",
    "    metric=\"cosine\", # distance metric for similarity search\n",
    "    spec=ServerlessSpec(\n",
    "        cloud=\"aws\",\n",
    "        region=\"us-east-1\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready the index\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsert vectors into Pinecone index in batches\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "# batches required to upsert all vectors else API throws error\n",
    "\n",
    "for i in range(0, len(vectors), BATCH_SIZE):\n",
    "    try:\n",
    "        batch = vectors[i: i + BATCH_SIZE]\n",
    "        index.upsert(vectors=batch)\n",
    "        print(\"Upserted batch starting at index:\", i)\n",
    "    except Exception as e:\n",
    "        # If the batch fails, upsert the vectors at lower batch size\n",
    "        BATCH_SIZE = 50\n",
    "        batch = vectors[i: i + BATCH_SIZE]\n",
    "        index.upsert(vectors=batch)\n",
    "        print(\"Upserted batch starting at index:\", i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query the VectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "import os\n",
    "\n",
    "# set up pinecone object\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"], environment=\"us-west1-gcp\", serverless=ServerlessSpec(\"us-west1-gcp\"))\n",
    "\n",
    "index_name = \"contentai\"\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# load the embedding model\n",
    "\n",
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# load the chatbot model\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "pipe = pipeline(\"text-generation\", model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                device_map=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example query\n",
    "query_text = \"What are the differences in the business of Tesla and Uber?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed the query text\n",
    "encoded_query = embedding_model.encode(query_text).tolist()\n",
    "\n",
    "k_responses = 2 # no of responses to fetch from each docs text\n",
    "\n",
    "goog_results = index.query(vector=encoded_query,\n",
    "                           filter={\"company\":\"GOOG\"},\n",
    "                           top_k=k_responses,\n",
    "                           include_metadata = True # include metadata to get the original text of the document\n",
    "                           )\n",
    "\n",
    "tsla_results = index.query(vector=encoded_query,\n",
    "                           filter={\"company\":\"TSLA\"},\n",
    "                           top_k=k_responses,\n",
    "                           include_metadata = True\n",
    "                           )\n",
    "\n",
    "uber_results = index.query(vector=encoded_query,\n",
    "                           filter={\"company\":\"UBER\"},\n",
    "                           top_k=k_responses,\n",
    "                           include_metadata = True\n",
    "                           )\n",
    "\n",
    "# get the context of the documents\n",
    "\n",
    "context = \"======== Google Document Context ========\\n\"\n",
    "for chunk in [matchs['metadata']['text'] for matchs in uber_results['matches']]:\n",
    "    context += chunk\n",
    "\n",
    "context += '''\\n======== Tesla Document Context ========\\n'''\n",
    "for chunk in [matchs['metadata']['text'] for matchs in tsla_results['matches']]:\n",
    "    context += chunk\n",
    "\n",
    "context += '''\\n======== Uber Document Context ========\\n'''\n",
    "for chunk in [matchs['metadata']['text'] for matchs in goog_results['matches']]:\n",
    "    context += chunk\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# ready the chatbot model\n",
    "\n",
    "model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to(\"cuda\")\n",
    "\n",
    "# prompt message for the chatbot\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are an AI assistant tasked with analyzing and comparing multiple PDF documents, specifically Form 10-K filings for Alphabet Inc., Tesla, Inc., and Uber Technologies, Inc. You are to provide clear, concise, and accurate responses for each query based on the extracted document content provided as context.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Document Content: {context}\\n\\n=======================User Query =======================\\n\\n\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"{query_text} \\n\\n=======================End Of User Query =======================\\n\\n\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"System Response\",\n",
    "        \"content\": \"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prepare the prompt manually\n",
    "prompt = \"\"\n",
    "for message in messages:\n",
    "    role = message[\"role\"]\n",
    "    content = message[\"content\"]\n",
    "    prompt += f\"{role}: {content}\\n\"\n",
    "\n",
    "# Encode the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids,\n",
    "                             max_new_tokens=512,\n",
    "                             min_new_tokens=256,\n",
    "                             temperature=1.0,\n",
    "                             repetition_penalty=1.2\n",
    "                             )\n",
    "\n",
    "# Decode the output\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
